{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# WordNetLemmatizer needs to be downloaded before use\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#To track function execution\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import spatial\n",
    "import networkx as nx\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('assignment_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SetID</th>\n",
       "      <th>Adverse Reactions</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a834d1cf-72fc-93bf-e053-2995a90a6191</td>\n",
       "      <td>The following adverse events were observed and...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a835b697-2beb-1ba8-e053-2995a90a470c</td>\n",
       "      <td>The following serious adverse reactions are de...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a837f13e-fafc-0535-e053-2995a90a5070</td>\n",
       "      <td>ADVERSE REACTIONS Clinical Trials Experience I...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a838204b-9564-9aa6-e053-2a95a90af02f</td>\n",
       "      <td>ADVERSE REACTIONS Clinical Trials Experience I...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f265e6dd-f47e-4511-9468-282184bcd1b1</td>\n",
       "      <td>The most common adverse reactions leading to d...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  SetID  \\\n",
       "0  a834d1cf-72fc-93bf-e053-2995a90a6191   \n",
       "1  a835b697-2beb-1ba8-e053-2995a90a470c   \n",
       "2  a837f13e-fafc-0535-e053-2995a90a5070   \n",
       "3  a838204b-9564-9aa6-e053-2a95a90af02f   \n",
       "4  f265e6dd-f47e-4511-9468-282184bcd1b1   \n",
       "\n",
       "                                   Adverse Reactions  Summary  \n",
       "0  The following adverse events were observed and...      NaN  \n",
       "1  The following serious adverse reactions are de...      NaN  \n",
       "2  ADVERSE REACTIONS Clinical Trials Experience I...      NaN  \n",
       "3  ADVERSE REACTIONS Clinical Trials Experience I...      NaN  \n",
       "4  The most common adverse reactions leading to d...      NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "990"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Adverse Reactions'].notnull()]\n",
    "df = df[df['Adverse Reactions'] != \"ERROR1\"]\n",
    "df = df[df['Adverse Reactions'] != \"Adverse Reactions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "989"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "for sentence in df['Adverse Reactions']:\n",
    "    sentences.append(sentence)\n",
    "    \n",
    "df['sentences'] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SetID</th>\n",
       "      <th>Adverse Reactions</th>\n",
       "      <th>Summary</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a834d1cf-72fc-93bf-e053-2995a90a6191</td>\n",
       "      <td>The following adverse events were observed and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The following adverse events were observed and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a835b697-2beb-1ba8-e053-2995a90a470c</td>\n",
       "      <td>The following serious adverse reactions are de...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The following serious adverse reactions are de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a837f13e-fafc-0535-e053-2995a90a5070</td>\n",
       "      <td>ADVERSE REACTIONS Clinical Trials Experience I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ADVERSE REACTIONS Clinical Trials Experience I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a838204b-9564-9aa6-e053-2a95a90af02f</td>\n",
       "      <td>ADVERSE REACTIONS Clinical Trials Experience I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ADVERSE REACTIONS Clinical Trials Experience I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f265e6dd-f47e-4511-9468-282184bcd1b1</td>\n",
       "      <td>The most common adverse reactions leading to d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The most common adverse reactions leading to d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  SetID  \\\n",
       "0  a834d1cf-72fc-93bf-e053-2995a90a6191   \n",
       "1  a835b697-2beb-1ba8-e053-2995a90a470c   \n",
       "2  a837f13e-fafc-0535-e053-2995a90a5070   \n",
       "3  a838204b-9564-9aa6-e053-2a95a90af02f   \n",
       "4  f265e6dd-f47e-4511-9468-282184bcd1b1   \n",
       "\n",
       "                                   Adverse Reactions  Summary  \\\n",
       "0  The following adverse events were observed and...      NaN   \n",
       "1  The following serious adverse reactions are de...      NaN   \n",
       "2  ADVERSE REACTIONS Clinical Trials Experience I...      NaN   \n",
       "3  ADVERSE REACTIONS Clinical Trials Experience I...      NaN   \n",
       "4  The most common adverse reactions leading to d...      NaN   \n",
       "\n",
       "                                           sentences  \n",
       "0  The following adverse events were observed and...  \n",
       "1  The following serious adverse reactions are de...  \n",
       "2  ADVERSE REACTIONS Clinical Trials Experience I...  \n",
       "3  ADVERSE REACTIONS Clinical Trials Experience I...  \n",
       "4  The most common adverse reactions leading to d...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Cleaning\n",
    "corpus = []\n",
    "def clean_content(df):\n",
    "    cleaned_content = []\n",
    "\n",
    "    for sent in tqdm(df[\"sentences\"]):\n",
    "        \n",
    "        #remove html content\n",
    "        review_content = BeautifulSoup(sent).get_text()\n",
    "            \n",
    "        #remove non-alphabetic characters\n",
    "        review_content = re.sub(\"[^a-zA-Z]\",\" \", review_content)\n",
    "    \n",
    "        #tokenize the sentences\n",
    "        words = word_tokenize(review_content.lower())\n",
    "    \n",
    "        #lemmatize each word to its lemma\n",
    "        lem = WordNetLemmatizer()\n",
    "        lemma_words = [lem.lemmatize(word) for word in words] \n",
    "        lemma_words = \" \".join(lemma_words)\n",
    "        cleaned_content.append(lemma_words)\n",
    "        \n",
    "        corpus.append(lemma_words)\n",
    "        \n",
    "    return(cleaned_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 989/989 [00:02<00:00, 347.44it/s]\n"
     ]
    }
   ],
   "source": [
    "cleaned_sentences = clean_content(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SetID</th>\n",
       "      <th>Adverse Reactions</th>\n",
       "      <th>Summary</th>\n",
       "      <th>sentences</th>\n",
       "      <th>cleaned_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a834d1cf-72fc-93bf-e053-2995a90a6191</td>\n",
       "      <td>The following adverse events were observed and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The following adverse events were observed and...</td>\n",
       "      <td>the following adverse event were observed and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a835b697-2beb-1ba8-e053-2995a90a470c</td>\n",
       "      <td>The following serious adverse reactions are de...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The following serious adverse reactions are de...</td>\n",
       "      <td>the following serious adverse reaction are des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a837f13e-fafc-0535-e053-2995a90a5070</td>\n",
       "      <td>ADVERSE REACTIONS Clinical Trials Experience I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ADVERSE REACTIONS Clinical Trials Experience I...</td>\n",
       "      <td>adverse reaction clinical trial experience in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a838204b-9564-9aa6-e053-2a95a90af02f</td>\n",
       "      <td>ADVERSE REACTIONS Clinical Trials Experience I...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ADVERSE REACTIONS Clinical Trials Experience I...</td>\n",
       "      <td>adverse reaction clinical trial experience in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f265e6dd-f47e-4511-9468-282184bcd1b1</td>\n",
       "      <td>The most common adverse reactions leading to d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The most common adverse reactions leading to d...</td>\n",
       "      <td>the most common adverse reaction leading to di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  SetID  \\\n",
       "0  a834d1cf-72fc-93bf-e053-2995a90a6191   \n",
       "1  a835b697-2beb-1ba8-e053-2995a90a470c   \n",
       "2  a837f13e-fafc-0535-e053-2995a90a5070   \n",
       "3  a838204b-9564-9aa6-e053-2a95a90af02f   \n",
       "4  f265e6dd-f47e-4511-9468-282184bcd1b1   \n",
       "\n",
       "                                   Adverse Reactions  Summary  \\\n",
       "0  The following adverse events were observed and...      NaN   \n",
       "1  The following serious adverse reactions are de...      NaN   \n",
       "2  ADVERSE REACTIONS Clinical Trials Experience I...      NaN   \n",
       "3  ADVERSE REACTIONS Clinical Trials Experience I...      NaN   \n",
       "4  The most common adverse reactions leading to d...      NaN   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  The following adverse events were observed and...   \n",
       "1  The following serious adverse reactions are de...   \n",
       "2  ADVERSE REACTIONS Clinical Trials Experience I...   \n",
       "3  ADVERSE REACTIONS Clinical Trials Experience I...   \n",
       "4  The most common adverse reactions leading to d...   \n",
       "\n",
       "                                   cleaned_sentences  \n",
       "0  the following adverse event were observed and ...  \n",
       "1  the following serious adverse reaction are des...  \n",
       "2  adverse reaction clinical trial experience in ...  \n",
       "3  adverse reaction clinical trial experience in ...  \n",
       "4  the most common adverse reaction leading to di...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_sentences'] = cleaned_sentences\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarising the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score_summary(vocabulary, stopwords_plus, text, processed_text):\n",
    "    # finding the co_occurance\n",
    "    \n",
    "    vocab_len = len(vocabulary)\n",
    "    weighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n",
    "\n",
    "    score = np.zeros((vocab_len),dtype=np.float32)\n",
    "    window_size = 3\n",
    "    covered_coocurrences = []\n",
    "\n",
    "    for i in range(0,vocab_len):\n",
    "        score[i]=1\n",
    "        for j in range(0,vocab_len):\n",
    "            if j==i:\n",
    "                weighted_edge[i][j]=0\n",
    "            else:\n",
    "                for window_start in range(0,(len(processed_text)-window_size+1)):\n",
    "\n",
    "                    window_end = window_start+window_size\n",
    "\n",
    "                    window = processed_text[window_start:window_end]\n",
    "\n",
    "                    if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
    "\n",
    "                        index_of_i = window_start + window.index(vocabulary[i])\n",
    "                        index_of_j = window_start + window.index(vocabulary[j])\n",
    "\n",
    "                        # index_of_x is the absolute position of the xth term in the window \n",
    "                        # (counting from 0) \n",
    "                        # in the processed_text\n",
    "\n",
    "                        if [index_of_i,index_of_j] not in covered_coocurrences:\n",
    "                            weighted_edge[i][j]+=1/math.fabs(index_of_i-index_of_j)\n",
    "                            covered_coocurrences.append([index_of_i,index_of_j])\n",
    "    \n",
    "    # Building inout\n",
    "    inout = np.zeros((vocab_len),dtype=np.float32)\n",
    "    for i in range(0,vocab_len):\n",
    "        for j in range(0,vocab_len):\n",
    "            inout[i]+=weighted_edge[i][j]\n",
    "    \n",
    "    # Calculating Score of each word using Pageranking\n",
    "    MAX_ITERATIONS = 50\n",
    "    d=0.85\n",
    "    threshold = 0.0001 #convergence threshold\n",
    "\n",
    "    for iter in range(0,MAX_ITERATIONS):\n",
    "        prev_score = np.copy(score)\n",
    "        for i in range(0,vocab_len):\n",
    "            summation = 0\n",
    "            for j in range(0,vocab_len):\n",
    "                if weighted_edge[i][j] != 0:\n",
    "                    summation += (weighted_edge[i][j]/inout[j])*score[j]\n",
    "\n",
    "            score[i] = (1-d) + d*(summation)\n",
    "            \n",
    "    # Generating phrases\n",
    "    \n",
    "    phrases = []\n",
    "    phrase = \" \"\n",
    "        \n",
    "    for word in text:\n",
    "        if word in stopwords_plus:\n",
    "            if phrase != \" \":\n",
    "                phrases.append(str(phrase).strip().split())\n",
    "            phrase = \" \"\n",
    "        elif word not in stopwords_plus:\n",
    "            phrase+=str(word)\n",
    "            phrase+=\" \"\n",
    "         \n",
    "    # Generating most related phrases\n",
    "#     print(\"Phrases\")\n",
    "#     print(phrases)\n",
    "    unique_phrases = []\n",
    "    for phrase in phrases:\n",
    "        if phrase not in unique_phrases:\n",
    "            unique_phrases.append(phrase)\n",
    "       \n",
    "#     print(unique_phrases)\n",
    "    # Shortning the phrases by removing lease significant phrases\n",
    "    for word in vocabulary:\n",
    "        for phrase in unique_phrases:\n",
    "            if (word in phrase) and ([word] in unique_phrases) and (len(phrase)>1):\n",
    "                unique_phrases.remove([word])\n",
    "                \n",
    "    # Scoring the phrases\n",
    "    phrase_scores = []\n",
    "    keywords = []\n",
    "#     print(vocabulary)\n",
    "#     print(unique_phrases)\n",
    "    for phrase in unique_phrases:\n",
    "        phrase_score=0\n",
    "        keyword = ''\n",
    "        for word in phrase:\n",
    "            keyword += str(word)\n",
    "            keyword += \" \"\n",
    "            phrase_score+=score[vocabulary.index(word)]\n",
    "        phrase_scores.append(phrase_score)\n",
    "        keywords.append(keyword.strip())\n",
    "        \n",
    "    # Generating the summary\n",
    "    sorted_index = np.flip(np.argsort(phrase_scores),0)\n",
    "    if len(keywords) >= 5:\n",
    "        keywords_num = 5\n",
    "    else:\n",
    "        keywords_num = len(keywords)\n",
    "#     keywords_num = len(keywords)    \n",
    "    \n",
    "    string_output = \"\"\n",
    "    for i in range(0,keywords_num):\n",
    "        string_output += str(keywords[sorted_index[i]]) + \",\\n\"\n",
    "        \n",
    "    return string_output[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_reaction(sentence):\n",
    "    \n",
    "    # Preparing the text for summarizing \n",
    "    text = sentence\n",
    "    text = text.split(\" \")\n",
    "    text = [i for i in text if i != \"\"]\n",
    "    \n",
    "    # Parts of speech tagging to identify unique stop words\n",
    "    POS_tag = nltk.pos_tag(text)\n",
    "    \n",
    "    # Customizing the stopword list\n",
    "    stopwords = []\n",
    "    wanted_POS = ['NN','NNS','NNP','NNPS','JJ','JJR','JJS','VBG','FW']\n",
    "\n",
    "    for word in POS_tag:\n",
    "        if word[1] not in wanted_POS:\n",
    "            stopwords.append(word[0])\n",
    "            \n",
    "    stopword_file = open(\"stopwords.txt\", \"r\")\n",
    "    \n",
    "    lots_of_stopwords = []\n",
    "    for line in stopword_file.readlines():\n",
    "        lots_of_stopwords.append(str(line.strip()))\n",
    "\n",
    "    stopwords_plus = []\n",
    "    stopwords_plus = stopwords + lots_of_stopwords\n",
    "    stopwords_plus = set(stopwords_plus)\n",
    "    \n",
    "    # Processing the text by removing the stop words\n",
    "    processed_text = []\n",
    "    for word in text:\n",
    "        if word not in stopwords_plus:\n",
    "            processed_text.append(word)\n",
    "    \n",
    "    # Creating vocubalory of words by removing the repeated words\n",
    "    vocabulary = list(set(processed_text))\n",
    "#     print(vocabulary)\n",
    "    summarised_sentence = extract_score_summary(vocabulary, stopwords_plus, text, processed_text)\n",
    "    \n",
    "    return summarised_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-91121840c8ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cleaned_sentences'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummarize_reaction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-0aaaf211292f>\u001b[0m in \u001b[0;36msummarize_reaction\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mvocabulary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m#     print(vocabulary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0msummarised_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_score_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopwords_plus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msummarised_sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-b237a524c4e4>\u001b[0m in \u001b[0;36mextract_score_summary\u001b[1;34m(vocabulary, stopwords_plus, text)\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[0mweighted_edge\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mwindow_start\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessed_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                     \u001b[0mwindow_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwindow_start\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'processed_text' is not defined"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for sentence in df['cleaned_sentences']:\n",
    "    summary.append(summarize_reaction(sentence))\n",
    "    count += 1\n",
    "    \n",
    "    if count == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
